# 量化

## 1.简介

### 1.1 Why

+ 硬件支持，这是由**硬件特性**提供的便利性

### 1.2 What

+ 量化的目标是求**scale**：
  + 量化：`int8V = float32V / scale`
  + 反量化：`fp32V = int8V * scale`

### 1.3 How

+ 求**scale**的方法
  + **min-max**：`scale = max(|T|) / 127`
  + **ADMM**：
  + **KL距离**：

+ **ADMM方法**：
  + 量化：`f(x) = x / s`
  + 反量化：`f-1(x) = x * s`
  + 量化后再反量化的Loss：`L = [E(x / s) * s - x] ** 2`
  + 最小化Loss，求L对s的偏导：`E(x / s) * [E(x / s) * s - x]`
  + 目标是：`E(x / s(k)) * s(k+1) - x = 0`
  + 每次求`E(x / s(k))`，它表示量化之后的结果
  + `s(k+1) = x * E(x / s(k)) / E(x / s(k))**2`
+ 注意事项：
  + 量化过程是针对每一层
  + 量化要准备Dataset，以CV为例，准备图片，有一个forward过程，然后每层都统计min-max，并算出每层的scale，提前写入文件。如果是KL量化，默认用2048个bin，调整bin个数，统计每层的输入的分布，然后映射到128(int8)，然后求KL距离，找到KL距离最小的T(bin个数)，那么量化要求的scale就是T/127；
  + 推理过程中weight是不变的，用scale量化后，保存；
  + int8的kernel实现：输入int8 * int8 得到输出int32，如果下一个神经网络不支持量化，就需要反量化int32->f32；如果下一个神经网络支持量化，就需要先反量化再量化int32->f32->int8；



## 2.TNN量化

+ 主函数：quantize.cc

  + 流程：
    1. 从input_path中导入DataSet
    2. 校准
  + 执行校准过程

  ```c
      //校准类对象
      Calibration calibration;
      //初始化校准器：获取一些神经网络相关句柄
      Status status = calibration.Init(net_config, model_config);
      if (status != TNN_OK) {
          printf("calibration init falied!\n");
          return -1;
      }
      //设置校准参数
      ret = calibration.SetCalibrationParams(cali_params);
      if (ret != 0) {
          printf("set calibration params falied!\n");
          return -1;
      }
      //执行校准
      status = calibration.RunCalibration(dataset);
      if (status != TNN_OK) {
          printf("calibration run falied!\n");
          return -1;
      }
  ```

  

+ 两个迭代过程：calibration.cc

  + 求weight：E(x / s(k)

  ```c
  //ADMM update更新量化结果
  static void UpdateQuantizedWeightsADMM(const float* weights, const int size,
                                         const int output_channel,
                                         float* weight_scale,
                                         const int quantize_bits,
                                         int8_t* quantized_weights) {
      const int oc_stride = size / output_channel;
      const float bound   = std::pow(2, quantize_bits - 1) - 1;
      const float eps     = 1e-9f;
      float weight_quan;
      ASSERT(quantize_bits > 4);
  
      for (int i = 0; i < size; i++) {
          //更新量化权重
          weight_quan = weights[i] / (weight_scale[i / oc_stride] + eps);//防止为0的情况出现
          quantized_weights[i] =
              std::min(bound, std::max(-bound, std::roundf(weight_quan)));
      }
  }
  ```

  

  + 求scale：

  ```c
  //更新权重缩放系数s(i+1)
  static void UpdateAlphaADMM(const float* weights, const int size,
                              const int output_channel, float* weight_scale,
                              int8_t* quantized_weights) {
      const int oc_stride = size / output_channel;
      const float eps     = 1e-9f;
      //循环更新weight_scale的值
      for (int i = 0; i < output_channel; i++) {
          const int offset = i * oc_stride;
          float sum1       = 0;
          float sum2       = 0;
  
          for (int j = 0; j < oc_stride; j++) {
              sum1 += weights[offset + j] * quantized_weights[offset + j];
              sum2 +=
                  quantized_weights[offset + j] * quantized_weights[offset + j];
          }
          //更新weight_scale值
          weight_scale[i] = sum1 / (sum2 + eps);
      }
  };
  ```

  

+ 注意事项：

  + 量化有两部分组成：
    + weight：已经量化后保存好了，不用计算了
    + input：scale是已经算好的，不用重复算；input / scale要计算
  + serialize：
    + 每个layer如果有serialize方法，就会把它的值进行保存，可以用protobuff或纯文本来保存
    + 神经网络，并不是每一层都支持量化，这是由kernel决定的
  + 嵌入式设备上使用量化多一些；CUDA Kernel有数据拷贝过程，量化效果不一定高；
  + 图片分布尽量一致，否则用输入的一部分batch图片统计的scale不准；
  + 如果不执行校准，会默认调用min-max方法，会造成浪费；
  + 关于TensorRT如果输入的是int8，后面没有强制int8，那么这个int8只是一个建议，后面运行时状态会根据本硬件支持的kernel来做。TensorRT动态选核能力(kernel实现)。
  + ADMM主要还是针对weight。
  + ADMM、min-max主要是用于权重校准
  
  + KL算法在input校准中用的比较多，因为与权重相比，输入特征的量不是很大
  
  ```c
  //计算输出blob的量化scale信息
  int Calibration::CalBlobScale(DataSet& dataset) {
      printf("Start to calculate blob scale ...\n");
      //获取网络资源句柄
      NetResource* net_resource = interpreter_->GetNetResource();
      
      //对整个网络层进行reshape
      Status status = instance_->Reshape(dataset.input_shape);
      if (status != TNN_OK) {
          LOGE("instance reshape falied!\n");
          return -1;
      }
  
      // Init Feature map
      int ret = InitFeatureMap();
      if (ret != 0) {
          LOGE("init feautre map for quantize falied!\n");
          return ret;
      }
      printf("\tInit Feature Map done!\n");
  
      // Collect the Range of Feature map
      ret = UpdateBlobRange(dataset);//循环每个文件，更新每个channel的最小值和最大值统计量
      if (ret != 0) {
          LOGE("collect feautre map range falied!\n");
          return ret;
      }
      printf("\tCollect Blob Range done!\n");
  
      // Calculate Distribute of Feature map
      ret = UpdateBlobDistribute(dataset);//再次读取dateset更新2048bin的统计
      if (ret != 0) {
          LOGE("update feautre map distribute falied!\n");
          return ret;
      }
      printf("\tCollect Blob Distribution done!\n");
  
      // Compute Scale of Feature map and save to resource map
      //直接循环需要计算scale的blob，通过遍历feature_map完成
      for (auto& item : feature_map_) {
          std::vector<float> scale_vec;
          //计算scale
          int ret = item.second->CalculateScale(scale_vec);
          if (ret != 0) {
              return ret;
          }
          
          std::string input_scale_name =
              item.first->GetBlobDesc().name + BLOB_SCALE_SUFFIX;
          //创建一个resource
          LayerResource* blob_scale_res = CreateIntScale(scale_vec);
          //讲resource添加到查找表
          net_resource->resource_map[input_scale_name] =
              std::shared_ptr<LayerResource>(blob_scale_res);
          printf("\t====> Calculate (%s) done!\n", input_scale_name.c_str());
      }
      return 0;
  }
  ```
  
  
  
  + 校准集用于input校准
  + calibration.cc/UpdateBlobDistribute，KL距离更新2048bin的统计过程，scale_calculator.cc/CalculateScalePerDis有计算KL距离的过程；
  + TensorRT中build engine的过程，就是在遍历寻找最适合硬件的kernel的过程；
  
  

## 3.参考资料

1. https://github.com/Tencent/TNN

