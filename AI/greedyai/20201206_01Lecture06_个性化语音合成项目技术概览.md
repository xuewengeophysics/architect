# 个性化语音合成项目技术概览

## 1. 大纲

### 1.1 目录

+ 个性化语音合成技术整体架构。
+ GE2E的设计与实现。
+ ️LSTM结构设计及Forward、Backward实现。️
+ BiLSTM的实现逻辑。

### 1.2 课前需要的知识点

+ 余弦距离的计算方法。
+ 了解LSTM在Tensorflow或Pytorch的使用方法。
+ 语音常用的语音特征FBank和MFCC的概念。

### 1.3 课前预习

+ 阅读论文：[Generalized END-TO-END Loss For Speaker Verification](https://arxiv.org/abs/1710.10467)
+ 阅读论文：[Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](https://arxiv.org/abs/1806.04558)
+ 阅读博文：[Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## 2. 主要内容

+ 项目阶段
  + 第一阶段：声纹；个性化其它方面有：发音的duration，emotion embedding，global style
  + 第二阶段：合成；语音模型，文本->频谱，Tacotron2，fast speech，deep voice(baidu)，attention
  + 第三阶段：声码器；80维的Mel谱特征->24000个样点的wave，上采样过程；conv；wavenet、wavernn、waveglow
+ 优化阶段
  + 计算图：算子拆解及合并；图的拓扑排序，神经网络的组织；
+ 语音项目通识
+ LSTM的代码实现(老师提供了2个版本)：慢速版本、优化版本(矩阵拼接)

## 3. 语音项目通识

### 3.1 基本概念

+ **声音的表示**：声音是一种波。常见的mp3、 wmv等格式都是压缩格式，必须转成非压缩的纯波形文件来处理，比如Windows PCM文件，也就是俗称的wav文件。 wav文件由存储的文件头和声音波形的采样点构成。  

+ **帧长**和**帧移**：对声音进行分析，需要对声音分帧，分帧操作一般不是简单的切开，而是使用移动窗函数来实现。帧与帧之间一般是有交叠的。一般情形每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠。我们称为以帧长25ms、帧移10ms分帧。 FFT，针对周期信号；长信号中的片段往往有一些周期性；

+ **人耳对声音频谱的响应是非线性的**。FBank特征提取要在预处理之后进行，这时语音已经分帧，我们需要逐帧提取FBank特征；每个字母的发音的时长不一样，语音识别中通常需要用动态规划来进行对齐，通常先用40维的特征进行粗对齐，再用80维的特征进行精细处理；

  ​	![image-20210102085242882](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20210102085242882.png)  

+ **声码器**：waveform->feature->encode and encryption->waveform。在发送端对语音信号进行分析，提取出语音信号的特征参量加以**编码**和**加密**，以取得和信道的匹配，经信息通道传递到接受端，再根据收到的特征参量恢复原始语音波形。waveform的时间域特征往往不明显，因此常采用频率域信号进行分析。

+ **声纹(VoicePrint)**：是用电声学仪器显示的携带言语信息的声波频谱。人类语言的产生是人体语言中枢与发音器官之间一个复杂的生理物理过程，人在讲话时使用的发声器官--舌、牙齿、喉头、肺、鼻腔在尺寸和形态方面每个人的差异很大，所以任何两个人的声纹图谱都有差异。每个人的语音声学特征既有相对稳定性，又有变异性，不是绝对的、一成不变的。这种变异可来自生理、病理、心理、模拟、伪装，也与环境干扰有关。voicefilter，从多人中过滤出需要的某个人的语音信号。

### 3.2 个性化语音合成整体架构

![image-20210102092000951](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20210102092000951.png)  

+ concat(Person Embedding和Text Embedding)

#### 3.2.1 Speaker Encoder(说话人特征编码器)

+ **实现对目标说话人发音特征提取**：说话人特征编码器具有**鉴别特定说话人**的功能。输入为短时自适应语音信号，输出为说话人在发音特征向量空间中的特征向量。该特征编码与语音对应的文本及背景噪音独立，是对说话人发音特质的描述。说话人特征编码器为语音合成模块提供个性化定制参考。
+ TE2E、GE2E(损失函数的技巧有点类似Triplet loss，让模型解决最难的问题)

#### 3.2.2 Synthesizer(合成器)

+ **实现文本到梅尔谱(mel spectrogram)的Seq2Seq变换**：

![image-20210102101304390](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20210102101304390.png)  

#### 3.2.3 Vocoder(声码器)

+ **频谱到波形的变换过程**：

+ 传统算法Griffin-lim算法：griffin-lim重建语音信号需要使用到幅度谱和相位谱。而MEL谱当中是不含相位信息的，因此griffin-lim在重建语音波形的时候只有MEL谱可以利用，利用帧与帧之间的关系估计出相位信息，重建语音波形。这里的MEL谱可以看做是实部，而相位信息可以看做是虚部，通过对实部和虚部的运算，得到最终的结果。

  1. 随机初始化一个相位谱；
  2. 用这个相位谱与已知的幅度谱(来自MEL谱)经过ISTFT(逆傅里叶变换)合成新的语音波形；
  3. 用合成语音做STFT，得到新的幅度谱和新的相位谱；
  4. 丢弃新的幅度谱，用已知幅度谱与新的相位谱合成新的语音；
  5. 重复2、3、4多次，直至合成的语音达到满意的效果或者迭代次数达到设定的上限。

+ **wavenet**：

  ​	![image-20210102104801195](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20210102104801195.png)  

