# 说话人特征提取技术实现

## 1. encoder的数据预处理方法

### 1.1 加载数据集

+ https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets

![image-20210103183933867](C:\Users\86138\AppData\Roaming\Typora\typora-user-images\image-20210103183933867.png)  

### 1.2 数据预处理

#### 1.2.1 代码位置

+ 代码入口：Real-Time-Voice-Cloning/encoder_preprocess.py
+ 声音预处理：Real-Time-Voice-Cloning/encoder/preprocess.py，3个预料的处理函数都是`_preprocess_speaker_dirs`；
+ 具体函数：Real-Time-Voice-Cloning/encoder/audio.py，`preprocess_wav`包含具体的3个处理步骤；

#### 1.2.2 步骤

1. **Load and preprocess the waveform**：加载语音数据，提取特征：

   1. 采样
   2. 音量处理
   3. 沉默处理：去除沉默段

   ```python
   #Real-Time-Voice-Cloning/encoder/audio.py
   def preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray],
                      source_sr: Optional[int] = None):
       """
       Applies the preprocessing operations used in training the Speaker Encoder to a waveform 
       either on disk or in memory. The waveform will be resampled to match the data hyperparameters.
   
       :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not 
       just .wav), either the waveform as a numpy array of floats.
       :param source_sr: if passing an audio waveform, the sampling rate of the waveform before 
       preprocessing. After preprocessing, the waveform's sampling rate will match the data 
       hyperparameters. If passing a filepath, the sampling rate will be automatically detected and 
       this argument will be ignored.
       """
       # Load the wav from disk if needed
       if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):
           wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)
       else:
           wav = fpath_or_wav
       
       # Resample the wav if needed
       if source_sr is not None and source_sr != sampling_rate:
           wav = librosa.resample(wav, source_sr, sampling_rate)
           
       # Apply the preprocessing: normalize volume and shorten long silences 
       wav = normalize_volume(wav, audio_norm_target_dBFS, increase_only=True)
       if webrtcvad:
           wav = trim_long_silences(wav)
       
       return wav
   ```

   

2. **Create the mel spectrogram**：输出Mel频谱

   ```python
   #Real-Time-Voice-Cloning/encoder/audio.py
   def wav_to_mel_spectrogram(wav):
       """
       Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.
       Note: this not a log-mel spectrogram.
       """
       frames = librosa.feature.melspectrogram(
           wav,
           sampling_rate,
           n_fft=int(sampling_rate * mel_window_length / 1000),
           hop_length=int(sampling_rate * mel_window_step / 1000),
           n_mels=mel_n_channels
       )
       return frames.astype(np.float32).T
   ```

   

### 1.3 保存数据

```python
#Real-Time-Voice-Cloning/encoder/preprocess.py
def _preprocess_speaker_dirs(speaker_dirs, dataset_name, datasets_root, out_dir, extension,
                             skip_existing, logger):
    print("%s: Preprocessing data for %d speakers." % (dataset_name, len(speaker_dirs)))
    
    # Function to preprocess utterances for one speaker
    def preprocess_speaker(speaker_dir: Path):
        # Give a name to the speaker that includes its dataset
        speaker_name = "_".join(speaker_dir.relative_to(datasets_root).parts)
        
        # Create an output directory with that name, as well as a txt file containing a 
        # reference to each source file.
        speaker_out_dir = out_dir.joinpath(speaker_name)
        speaker_out_dir.mkdir(exist_ok=True)
        sources_fpath = speaker_out_dir.joinpath("_sources.txt")
        
        # There's a possibility that the preprocessing was interrupted earlier, check if 
        # there already is a sources file.
        if sources_fpath.exists():
            try:
                with sources_fpath.open("r") as sources_file:
                    existing_fnames = {line.split(",")[0] for line in sources_file}
            except:
                existing_fnames = {}
        else:
            existing_fnames = {}
        
        # Gather all audio files for that speaker recursively
        sources_file = sources_fpath.open("a" if skip_existing else "w")
        for in_fpath in speaker_dir.glob("**/*.%s" % extension):
            # Check if the target output file already exists
            out_fname = "_".join(in_fpath.relative_to(speaker_dir).parts)
            out_fname = out_fname.replace(".%s" % extension, ".npy")
            if skip_existing and out_fname in existing_fnames:
                continue
                
            # Load and preprocess the waveform
            wav = audio.preprocess_wav(in_fpath)
            if len(wav) == 0:
                continue
            
            # Create the mel spectrogram, discard those that are too short
            frames = audio.wav_to_mel_spectrogram(wav)
            if len(frames) < partials_n_frames:
                continue
            
            out_fpath = speaker_out_dir.joinpath(out_fname)
            np.save(out_fpath, frames)
            logger.add_sample(duration=len(wav) / sampling_rate)
            sources_file.write("%s,%s\n" % (out_fname, in_fpath))
        
        sources_file.close()
    
    # Process the utterances for each speaker
    with ThreadPool(8) as pool:
        list(tqdm(pool.imap(preprocess_speaker, speaker_dirs), dataset_name, len(speaker_dirs),
                  unit="speakers"))
    logger.finalize()
    print("Done preprocessing %s.\n" % dataset_name)
```



## 2.相似度矩阵的计算

+ Real-Time-Voice-Cloning/encoder/model.py